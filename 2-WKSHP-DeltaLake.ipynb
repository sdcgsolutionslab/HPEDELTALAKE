{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b3e6c5",
   "metadata": {},
   "source": [
    "## Build a house on the lake - Lab 2\n",
    "\n",
    "---\n",
    "Today, as organizations collects large amounts of data of any type (structured, unstructured, semi-structured), the vast majority of enterprise data lands in **Data Lakes**. Different software system, analytics applications consume data directly through the Data Lake. In our environment, the data that reside on the Data Lake is managed by HPE Ezmeral Data Fabric. The Data Lake can store any type of data (structured, unstructured, semi-structured). \n",
    "\n",
    "To accomplish both data analytics workloads (such as AI/ML) and Business Intelligence (BI) use cases, organizations typically have to build a **Data Lakehouse** on top of Data Lake which serves as data storage. A Data Lakehouse is an open architecture that combines the best elements of data lakes and data warehouses. Data Lakehouse overcomes critical Data Lake limitations such as poor performance, no support for transactions and no enforcement of data quality.\n",
    "\n",
    "In this part of the lab, you will explore how to build a data Lakehouse with **Delta Lake** and how to access data. \n",
    "\n",
    "Delta Lake is a key technology used to implement a Data Lakehouse that enables storing any type of data once in a data lake and doing AI and BI on that data directly.\n",
    "\n",
    "Delta Lake implements key functions such as ACID properties for atomic transactions, time travel (easily move data to any of its version in the timeline through the transaction logs), open format (Delta Lake uses the open format ***Apache Parquet*** to store the data), governance (access controls, audit logging), data access (Spark SQL can be used to query the data).\n",
    "\n",
    "Delta Lake is primarily based on 2 technologies:\n",
    "* Apache Spark (analytics engine for large-scale data processing both for batch and streaming modes)\n",
    "* Apache Parquet (columnar storage format most suitable to store and read data for analytics purpose. Technology like Spark SQL can be used to query the data). Apache Parquet format is used to store all the metadata used for data quality, ACID transaction, versioning, and governance.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"build-the-lakehouse.png\" height=\"60%\" width=\"60%\" align=\"right\">\n",
    "\n",
    "### Here we are at the edge of our lake and our foundation has been poured using HPE Ezmeral as the concrete\n",
    "In the diagram you see that you are using one of the Jupyter Notebook applications managed by HPE Ezmeral Runtime Enterprise in a Kubernetes cluster. In that cluster is the tenant or Kubernetes namespace that your studentID gives you access to. Other students have access to the same tenant, but they are running their own \"Jupyter Notebook with ML Toolkits\" applications.\n",
    "\n",
    "Next you can see on the right that the data in our Data Lake is being managed by HPE Ezmeral Data Fabric.  We have configured the HPE Ezmeral Data Fabric to provide all of our tenant storage for us. In this lab we are using a sandbox storage space that is called \"TenantStorage\". This is in fact a default DataTap setup automatically by HPE Ezmeral Runtime Enterprise when the cluster and tenant were created.\n",
    "\n",
    "We can also create DataTaps to other HDFS or MAPRFS existing Data Lakes if we want to.  For this lab we will just use the default TenantStorage sandbox DataTap.  Each Student using this workshop gets their own folder inside the shared TenantStorage. Each Student has a copy of the source data pre-copied into their student folder. That source data - a parquet file will be used to create a Delta Lake table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383cefd-aca5-43f6-88e7-2ee8158177d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%kubeRefresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f522a10-8208-4544-94da-11c04508a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kubectl config use-context k8scluster2-DeltaLake-student954\n",
    "kubectl config current-context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92cf26-d635-4899-9ab7-6b46efc729b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f45da-ca28-466a-86ac-be8b1c19696c",
   "metadata": {},
   "source": [
    "# Create your studentID folder under the default shared DataTap storage and copy the Parquet data file\n",
    "The default TenantStorage Datatap is shared among all the workshop participants. The participants (studentID) will get their own folder (/studentID) under the shared datatap storage (dtap://TenantStorage/studentID/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f739b-82ce-4f9a-8c43-c4cdc934a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "userID=\"student954\"\n",
    "#hadoop version\n",
    "hadoop fs -rm -f -r dtap://TenantStorage/${userID}\n",
    "hadoop fs -mkdir dtap://TenantStorage/${userID}\n",
    "hadoop fs -put SAISEU19-loan-risks.snappy.parquet dtap://TenantStorage/${userID}/.\n",
    "hadoop fs -ls dtap://TenantStorage/${userID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167b91b-a109-4784-b1b6-0ffcedca3946",
   "metadata": {},
   "source": [
    "# Truly Magic\n",
    "<img src=\"its-magic.png\" height=\"60%\" width=\"60%\" align=\"right\">\n",
    "\n",
    "### Great things in small packages\n",
    "The 1st cell in this notebook does some really helpful things for you. Jupyter Notebooks have many standard \"magic\" commands to make your code more integrated with various other environments, such as running inline OS or BASH commands within a given kernel. HPE Ezmeral ML Ops includes this %%spark command to quickly get you connected to the Livy server that is already deployed for you in your tenant.  When you execute the below %%spark magic command, you will get a Livy managed spark session running in your tenant with a Livy Spark driver and two executor pods.\n",
    "\n",
    "The rest of the diagram shows you the environment that this Jupyter Notebook with ML Toolkits kubernetes cluster based application is running in.  You are interacting with a Jupyter Notebook. That notebook has access to HPE Ezmeral Runtime Enterprise DataTaps including our default \"sandbox TenantStorage\".  Illustrated in this diagram is an example of also tapping into other data sources world wide.  This lab only uses the local tenantstorage sandbox however."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b4c9f-fb15-4fc3-a76b-83354cfc1efd",
   "metadata": {},
   "source": [
    "# Run the %%spark cell magic\n",
    "When you run this cell magic command the HPE Ezmeral Runtime Enterprise custom logic will scan the Tenant for any Livy endpoints currently deployed. The URL will be output in an interactive prompt below this cell. Select the URL with your mouse and copy it into your clipboard using control-c or command-c depending on your client OS. Then, paste that URL into the provided edit box.  Next, ***as Livy authenticates users with LDAP credentials***, you will be required to enter your Student Password. After you enter your password and press enter, it will take a little over 1 minute to start a dynamic Spark cluster ***(1 Spark driver POD and 2 Spark executor PODs for the duration of the Spark Livy session)*** using REST API calls to the Livy REST endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4fe481",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505b033-0fb3-491b-9cf1-6757c9515bdc",
   "metadata": {},
   "source": [
    "# Customize our Livy, Spark session specifically to include Delta Lake libraries\n",
    "We now have a default Spark session running. We could start running many Spark commands. However, we also want to run Delta Lake extensions to PySpark.  To do this we will further customize our Spark environment. This next cell uses another custom Jupyter Magic command to re-configure our Spark session and add in support for Delta lake extensions. Simply run this cell and after another minute we will have a new session ready to accept Pyspark, Delta Lake commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4de11-db2f-4224-8a54-696ed6d0e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\": {\n",
    "            \"spark.ssl.enabled\": false,\n",
    "            \"spark.hadoop.fs.dtap.impl\": \"com.bluedata.hadoop.bdfs.Bdfs\",\n",
    "            \"spark.hadoop.fs.AbstractFileSystem.dtap.impl\": \"com.bluedata.hadoop.bdfs.BdAbstractFS\",\n",
    "            \"spark.hadoop.fs.dtap.impl.disable.cache\": \"false\",\n",
    "            \"spark.driver.extraClassPath\": \"local:///opt/bdfs/bluedata-dtap.jar\",\n",
    "            \"spark.executor.extraClassPath\": \"local:///opt/bdfs/bluedata-dtap.jar\",\n",
    "            \"spark.kubernetes.driver.label.hpecp.hpe.com/dtap\": \"hadoop2\",\n",
    "            \"spark.kubernetes.executor.label.hpecp.hpe.com/dtap\": \"hadoop2\",\n",
    "            \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n",
    "            \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e27262-df90-445c-9b29-b74ffe524de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9212f95-0cf9-47d4-8e97-a89bf31b4130",
   "metadata": {},
   "source": [
    "# Setup our source file\n",
    "Use HDFS commands to copy our parquet file out to our tenant storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c57cb84-2b5e-42e1-a804-0891de1f705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "home = !echo $HOME\n",
    "home = home[0]\n",
    "import subprocess\n",
    "fpath = 'dtap://TenantStorage/student954/SAISEU19-loan-risks.snappy.parquet'\n",
    "#subprocess.check_output(['hadoop', 'fs', '-mkdir', '-p', 'dtap://TenantStorage/deltalake-workshop'])\n",
    "#subprocess.check_output(['hadoop', 'fs', '-test', '-e', fpath])\n",
    "print(f'Home [{home}] fpath [{fpath}] ')\n",
    "try:\n",
    "    #subprocess.check_output(['hadoop', 'fs', '-e', fpath])\n",
    "    subprocess.check_output(['hadoop', 'fs', '-test', '-e', fpath])\n",
    "    print(f\"{fpath} exists\")\n",
    "except:\n",
    "    print(f\" error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193470f4",
   "metadata": {},
   "source": [
    "## 1.Load our source Parquet file and create a Parquet Table \"View\" out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d112d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"dtap://TenantStorage/student954/deltalake-workshop/\"\n",
    "\n",
    "spark.read.format(\"parquet\").load(\"dtap://TenantStorage/student954/SAISEU19-loan-risks.snappy.parquet\") \\\n",
    "  .write.format(\"parquet\").save(parquet_path)\n",
    "\n",
    "print(\"Created a Parquet table at \" + parquet_path)\n",
    "\n",
    "# Create a view on the table called loans_parquet\n",
    "spark.read.format(\"parquet\").load(parquet_path).createOrReplaceTempView(\"loans_parquet\")\n",
    "print(\"Defined view 'loans_parquet'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Let's explore this parquet table.\n",
    "# *Schema of the table*\n",
    "# - load_id - unique id for each loan\n",
    "# - funded_amnt - principal amount of the loan funded to the loanee\n",
    "# - paid_amnt - amount from the principle that has been paid back (ignoring interests)\n",
    "# - addr_state - state where this loan was funded\n",
    "\n",
    "spark.sql(\"select * from loans_parquet\").show()\n",
    "\n",
    "spark.sql(\"select count(*) from loans_parquet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219be900",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Let's start appending some new data to it using Structured Streaming.**\n",
    "- We will generate a stream of data from with randomly generated loan ids and amounts. \n",
    "- In addition, we are going to define a few more useful utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def random_checkpoint_dir(): \n",
    "    return \"dtap://TenantStorage/deltalake-workshop/student954-yyZdKqMe/chkpt/%s\" % str(random.randint(0, 10000))\n",
    "\n",
    "# User-defined function to generate random state\n",
    "\n",
    "states = [\"CA\", \"TX\", \"NY\", \"IA\"]\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def random_state():\n",
    "    return str(random.choice(states))\n",
    "\n",
    "# Function to start a streaming query with a stream of randomly generated load data and append to the parquet table\n",
    "def generate_and_append_data_stream(table_format, table_path):\n",
    "  \n",
    "    stream_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load() \\\n",
    "                .withColumn(\"loan_id\", 10000 + col(\"value\")) \\\n",
    "                .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \\\n",
    "                .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \\\n",
    "                .withColumn(\"addr_state\", random_state())\n",
    "    query = stream_data.writeStream.format(table_format) \\\n",
    "                .option(\"checkpointLocation\", random_checkpoint_dir()) \\\n",
    "                .trigger(processingTime = \"10 seconds\") \\\n",
    "                .start(table_path)\n",
    "    return query\n",
    "\n",
    "# Function to stop all streaming queries \n",
    "import os\n",
    "def stop_all_streams():\n",
    "    # Stop all the streams\n",
    "    print(\"Stopping all streams\")\n",
    "    for s in spark.streams.active:\n",
    "        s.stop()\n",
    "    print(\"Stopped all streams\")\n",
    "#     print(\"Deleting checkpoints\")  \n",
    "#     try:\n",
    "#         deleteS3Folder(\"delta\",\"loans-demo/chkpt\")\n",
    "#     except S3Error as err:\n",
    "#         print(str(err))\n",
    "#     print(\"Deleted checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start a new stream to append data to the Parquet table\n",
    "from time import sleep\n",
    "\n",
    "stream_query = generate_and_append_data_stream(\n",
    "    table_format = \"parquet\", \n",
    "    table_path = parquet_path)\n",
    "sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if the data is being added to the table or not\n",
    "spark.read.format(\"parquet\").load(parquet_path).count()\n",
    "\n",
    "# Where did our existing 14705 rows go? Let's see the data once again\n",
    "spark.read.format(\"parquet\").load(parquet_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9e139",
   "metadata": {},
   "source": [
    "Where did the two new columns `timestamp` and `value` come from? What happened here!\n",
    "\n",
    "What really happened is that when the streaming query started adding new data to the Parquet table, it did not properly account for the existing data in the table. Furthermore, the new data files that written out accidentally had two extra columns in the schema. Hence, when reading the table, the 2 different schema from different files were merged together, thus unexpectedly modifying the schema of the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_all_streams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89dfa8",
   "metadata": {},
   "source": [
    "## 2. Batch + stream processing and schema enforcement with Delta Lake\n",
    "- Let's understand Delta Lake solves these particular problems (among many others). We will start by creating a Delta table from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Delta Lake Silver Path\n",
    "delta_path = \"dtap://TenantStorage/deltalake-workshop/student954-yyZdKqMe/loans-delta\"\n",
    "\n",
    "spark.sql(\"set spark.sql.shuffle.partitions = 1\")\n",
    "spark.sql(\"set spark.databricks.delta.snapshotPartitions = 1\")\n",
    "\n",
    "# Create the Delta table with the same loans data\n",
    "spark.read.format(\"parquet\").load(\"dtap://TenantStorage/student954/SAISEU19-loan-risks.snappy.parquet\").write.format(\"delta\").save(delta_path)\n",
    "print(\"Created a Delta table at \" + delta_path)\n",
    "\n",
    "spark.read.format(\"delta\").load(delta_path).createOrReplaceTempView(\"loans_delta\")\n",
    "print(\"Defined view 'loans_delta'\")\n",
    "\n",
    "spark.sql(\"select count(*) from loans_delta\").show()\n",
    "\n",
    "spark.sql(\"select * from loans_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221369ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run a streaming count(*) on the table so that the count updates automatically\n",
    "spark.readStream.format(\"delta\").load(delta_path).createOrReplaceTempView(\"loans_delta_stream\")\n",
    "spark.sql(\"select count(*) from loans_delta_stream\")\n",
    "\n",
    "# Now let's try writing the streaming appends once again\n",
    "stream_query_2 = generate_and_append_data_stream(table_format = \"delta\", table_path = delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2faa50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The writes were blocked because the schema of the new data did not match the schema of table\n",
    "# **Now, let's fix the streaming query by selecting the columns we want to write.**\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "# Generate a stream of randomly generated load data and append to the parquet table\n",
    "def generate_and_append_data_stream_fixed(table_format, table_path):\n",
    "    \n",
    "    stream_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 50).load() \\\n",
    "            .withColumn(\"loan_id\", 10000 + col(\"value\")) \\\n",
    "            .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \\\n",
    "            .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \\\n",
    "            .withColumn(\"addr_state\", random_state()) \\\n",
    "            .select(\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\")   # *********** FIXED THE SCHEMA OF THE GENERATED DATA *************\n",
    "\n",
    "    query = stream_data.writeStream.format(table_format) \\\n",
    "            .option(\"checkpointLocation\", random_checkpoint_dir()) \\\n",
    "            .trigger(processingTime=\"10 seconds\") \\\n",
    "            .start(table_path)\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can successfully write to the table. Note the count in the above streaming query increasing as we write to this table.\n",
    "stream_query_2 = generate_and_append_data_stream_fixed(table_format = \"delta\", table_path = delta_path)\n",
    "sleep(10)\n",
    "\n",
    "# In fact, we can run multiple concurrent streams writing to that table, it will work together.\n",
    "stream_query_3 = generate_and_append_data_stream_fixed(table_format = \"delta\", table_path = delta_path)\n",
    "\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for sanity check, let's query as a batch\n",
    "spark.sql(\"select count(*) from loans_delta\").show()\n",
    "\n",
    "stop_all_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b84698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Schema Evolution\n",
    "# - Let's evolve the schema of the table\n",
    "# - We will run a batch query that will\n",
    "# - Append some new loans\n",
    "# - Add a boolean column 'closed' that signifies whether the loan has been closed and paid off or not.\n",
    "# - We are going to set the option `mergeSchema` to `true` to force the evolution of the Delta table's schema.\n",
    "# \n",
    "cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n",
    "\n",
    "items = [\n",
    "  (1111111, 1000, 1000.0, 'TX', True), \n",
    "  (2222222, 2000, 0.0, 'CA', False)\n",
    "]\n",
    "\n",
    "loan_updates = spark.createDataFrame(items, cols) \\\n",
    ".withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\"))\n",
    "  \n",
    "loan_updates.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "spark.read.format(\"delta\").load(delta_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e585c1",
   "metadata": {},
   "source": [
    "## 3. Delete from Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the number of fully paid loans.\n",
    "spark.sql(\"SELECT COUNT(*) FROM loans_delta WHERE funded_amnt = paid_amnt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "#sc.addPyFile(\"/opt/spark/jars/delta-core_2.12-1.0.0.jar\")\n",
    "sc.addPyFile(\"/opt/mapr/spark/spark-3.1.2/jars/delta-core_2.12-1.0.0.jar\")\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "deltaTable.delete(\"funded_amnt = paid_amnt\")\n",
    "\n",
    "# Let's check the number of fully paid loans once again.\n",
    "spark.sql(\"SELECT COUNT(*) FROM loans_delta WHERE funded_amnt = paid_amnt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63317921",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cdad4d",
   "metadata": {},
   "source": [
    "## 4. Audit Delta Lake Table History\n",
    "- All changes to the Delta table are recorded as commits in the table's transaction log. As you write into a Delta table or directory, every operation is automatically versioned. You can use the HISTORY command to view the table's history. For more information, check out the [docs](https://docs.delta.io/latest/delta-utility.html#history).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a89846",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773864de",
   "metadata": {},
   "source": [
    "## 5. Travel back in time\n",
    "- Delta Lake’s time travel feature allows you to access previous versions of the table. Here are some possible uses of this feature:\n",
    "    - Auditing Data Changes\n",
    "    - Reproducing experiments & reports\n",
    "    - Rollbacks\n",
    " \n",
    "You can query by using either a timestamp or a version number using Python, Scala, and/or SQL syntax. For this examples we will query a specific version using the Python syntax.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's query the table's state before we deleted the data, which still contains the fully paid loans.\n",
    "previousVersion = deltaTable.history(1).select(\"version\").collect()[0][0] - 1\n",
    "\n",
    "spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", previousVersion) \\\n",
    "    .load(delta_path) \\\n",
    "    .createOrReplaceTempView(\"loans_delta_pre_delete\")\n",
    "\n",
    "# We see the same number of fully paid loans that we had seen before delete.\n",
    "spark.sql(\"SELECT COUNT(*) FROM loans_delta_pre_delete WHERE funded_amnt = paid_amnt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71edbe7",
   "metadata": {},
   "source": [
    "## 6. Vacuum old versions of Delta Lake tables\n",
    "- While it's nice to be able to time travel to any previous version, sometimes you want actually delete the data from storage completely for reducing storage costs or for compliance reasons (example, GDPR).\n",
    "- The Vacuum operation deletes data files that have been removed from the table for a certain amount of time. For more information, check out the [docs](https://docs.delta.io/latest/delta-utility.html#vacuum).\n",
    "- By default, `vacuum()` retains all the data needed for the last 7 days. For this example, since this table does not have 7 days worth of history, we will retain 0 hours, which means to only keep the latest state of the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965dfa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")\n",
    "deltaTable.vacuum(retentionHours = 0)\n",
    "\n",
    "print(\"previousversion:{previousVersion}\")\n",
    "\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", previousVersion).load(delta_path).createOrReplaceTempView(\"loans_delta_pre_delete\")\n",
    "\n",
    "# Same query as before, but it now fails\n",
    "try:\n",
    "    spark.sql(\"SELECT COUNT(*) FROM loans_delta_pre_delete WHERE funded_amnt = paid_amnt\").show()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533dd45c",
   "metadata": {},
   "source": [
    "## 7. Upsert into Delta Lake table using Merge\"\n",
    "\n",
    "You can upsert data from an Apache Spark DataFrame into a Delta Lake table using the merge operation. This operation is similar to the SQL MERGE command but has additional support for deletes and extra conditions in updates, inserts, and deletes. For more information checkout the [docs](https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge).\n",
    "\n",
    "With a legacy data pipeline, to insert or update a table, you must:\n",
    "1. Identify the new rows to be inserted\n",
    "2. Identify the rows that will be replaced (i.e. updated)\n",
    "3. Identify all of the rows that are not impacted by the insert or update\n",
    "4. Create a new temp based on all three insert statements\n",
    "5. Delete the original table (and all of those associated files)\n",
    "6. \"Rename\" the temp table back to the original table name\n",
    "7. Drop the temp table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Delta Lake Silver Path\n",
    "delta_small_path = \"dtap://TenantStorage/deltalake-workshop/student954-yyZdKqMe/loans-delta-small\"\n",
    "\n",
    "# Create the Delta table with the same loans data\n",
    "spark.read.format(\"parquet\").load(\"dtap://TenantStorage/student954/SAISEU19-loan-risks.snappy.parquet\") \\\n",
    "    .where(\"loan_id < 3\") \\\n",
    "    .write.format(\"delta\").save(delta_small_path)\n",
    "print(\"Created a Delta table at \" + delta_small_path)\n",
    "\n",
    "spark.read.format(\"delta\").load(delta_small_path).createOrReplaceTempView(\"loans_delta_small\")\n",
    "print(\"Defined view 'loans_delta_small'\")\n",
    "\n",
    "# Let's focus only on a part of the loans_delta table\n",
    "spark.sql(\"select * from loans_delta_small order by loan_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42268509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Now, let's say we got some new loan information**\n",
    "# 1. Existing loan_id = 2 has been fully repaid. The corresponding row needs to be updated.\n",
    "# 2. New loan_id = 3 has been funded in CA. This is need to be inserted as a new row.\n",
    "\n",
    "cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state']\n",
    "items = [\n",
    "  (2, 1000, 1000.0, 'TX'), # existing loan's paid_amnt updated, loan paid in full\n",
    "  (3, 2000, 0.0, 'CA')     # new loan's details\n",
    "]\n",
    "\n",
    "loan_updates = spark.createDataFrame(items, cols)\n",
    "\n",
    "loan_updates.show()\n",
    "\n",
    "\n",
    "# **Merge can upsert this in a single atomic operation.** \n",
    "#  \n",
    "# SQL `MERGE` command can do both `UPDATE` and `INSERT`.\n",
    "# ``` \n",
    "# MERGE INTO target t\n",
    "# USING source s\n",
    "# WHEN MATCHED THEN UPDATE SET ...\n",
    "# WHEN NOT MATCHED THEN INSERT ....\n",
    "# ```\n",
    "\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, delta_small_path)\n",
    "\n",
    "delta_table.alias(\"t\").merge(\n",
    "    loan_updates.alias(\"s\"), \n",
    "    \"s.loan_id = t.loan_id\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()\n",
    "\n",
    "spark.sql(\"select * from loans_delta_small order by loan_id\").show()\n",
    "\n",
    "# **Note the changes in the table**\n",
    "# - Existing loan_id = 2 should have been updated with paid_amnt set to 1000. \n",
    "# - New loan_id = 3 have been inserted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026dbb3",
   "metadata": {},
   "source": [
    "## 8. Advanced uses of Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf81fe3",
   "metadata": {},
   "source": [
    "### 8.1 Streaming upserts into a Delta Lake table using merge and foreachBatch\n",
    "- You can continuously upsert from a streaming query output using merge inside `foreachBatch` operation of Structured Streaming.\n",
    "- Let's say, we want to maintain a count of the loans per state in a table, and new loans arrive, we want to update the counts.\n",
    "- To do this, we will first initialize the table and a few associated UDFs and configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a stream of randomly generated load data and append to the parquet table\n",
    "import random\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "loan_counts_by_states_path = \"dtap://TenantStorage/deltalake-workshop/student954-yyZdKqMe/loans-by-states\"\n",
    "chkpt_path = \"dtap://TenantStorage/deltalake-workshop/student954-yyZdKqMe/chkpt/%s\" % str(random.randint(0, 10000))\n",
    "\n",
    "\n",
    "# Initialize the table\n",
    "spark.createDataFrame([ ('CA', '0') ], [\"addr_state\" , \"count\"]).write.format(\"delta\").mode(\"overwrite\").save(loan_counts_by_states_path)\n",
    "\n",
    "\n",
    "# User-defined function to generate random state\n",
    "states = [\"CA\", \"TX\", \"NY\", \"IA\"]\n",
    "@udf(returnType=StringType())\n",
    "def random_state():\n",
    "    return str(random.choice(states))\n",
    "\n",
    "# Define the function to be called on the output of each micro-batch. \n",
    "# This function will use merge to upsert into the Delta table.\n",
    "\n",
    "loan_counts_by_states_table = DeltaTable.forPath(spark, loan_counts_by_states_path)\n",
    "\n",
    "# Function to upsert per-state counts generated in each microbatch of a streaming query\n",
    "# - updated_counts_df = the updated counts generated from a microbatch\n",
    "def upsert_state_counts_into_delta_table(updated_counts_df, batch_id):\n",
    "    loan_counts_by_states_table.alias(\"t\").merge(\n",
    "      updated_counts_df.alias(\"s\"), \n",
    "      \"s.addr_state = t.addr_state\") \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "# Define and run the streaming query using this function with `foreachBatch`.\n",
    "# loan_ids that have been complete paid off, random generated\n",
    "loans_update_stream_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load() \\\n",
    "        .withColumn(\"loan_id\", rand() * 100) \\\n",
    "        .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \\\n",
    "        .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \\\n",
    "        .withColumn(\"addr_state\", random_state()) \\\n",
    "        .createOrReplaceTempView(\"generated_loads\")\n",
    "\n",
    "# use foreachBatch to define what to do with each output micro-batch DataFrame\n",
    "query = spark.sql(\"select addr_state, count(*) as count from generated_loads group by addr_state\") \\\n",
    "      .writeStream.format(\"delta\").foreachBatch(upsert_state_counts_into_delta_table) \\\n",
    "      .option(\"checkpointLocation\", chkpt_path) \\\n",
    "      .trigger(processingTime = '3 seconds') \\\n",
    "      .outputMode(\"update\") \\\n",
    "      .start(loan_counts_by_states_path)\n",
    "\n",
    "sleep(30)\n",
    "# Let's query the state to see the counts. If you run the following cell repeatedly, \n",
    "# you will see that the counts will keep growing.\n",
    "\n",
    "spark.read.format(\"delta\").load(loan_counts_by_states_path).orderBy(\"addr_state\").show()\n",
    "\n",
    "stop_all_streams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1305c",
   "metadata": {},
   "source": [
    "### 8.2 Deduplication using `insert-only` merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac126ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "delta_path = \"dtap://TenantStorage/deltalake-workshop/student954-yyZdKqMe/loans-delta2\"\n",
    "\n",
    "# Define new loans table data\n",
    "data = [\n",
    "  (0, 1000, 1000.0, 'TX'), \n",
    "  (1, 2000, 0.0, 'CA')\n",
    "]\n",
    "cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state']\n",
    "\n",
    "# Write a new Delta Lake table with the loans data\n",
    "spark.createDataFrame(data, cols).write.format(\"delta\").save(delta_path)\n",
    "\n",
    "# Define DeltaTable object\n",
    "dt = DeltaTable.forPath(spark, delta_path)\n",
    "dt.toDF().show()\n",
    "\n",
    "# Define a DataFrame containing new data, some of which is already present in the table\n",
    "new_data = [  \n",
    "  (1, 2000, 0.0, 'CA'),    # duplicate, loan_id = 1 is already present in table and don't want to update\n",
    "  (2, 5000, 1010.0, 'NY')  # new data, not present in table\n",
    "]\n",
    "new_data_df = spark.createDataFrame(new_data, cols)\n",
    "\n",
    "new_data_df.show()\n",
    "\n",
    "# Run \"insert-only\" merqe query (i.e., no update clause)\n",
    "dt = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "dt.alias(\"t\").merge(\n",
    "    new_data_df.alias(\"s\"), \n",
    "    \"s.loan_id = t.loan_id\") \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()\n",
    "\n",
    "dt.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec00fb",
   "metadata": {},
   "source": [
    "## 9. Tutorial Summary\n",
    "Full support for batch and streaming workloads\n",
    "- Delta Lake allows batch and streaming workloads to concurrently read and write to Delta Lake tables with full ACID transactional guarantees.\n",
    "\n",
    "Schema enforcement and schema evolution\n",
    "- Delta Lake provides the ability to specify your schema and enforce it. This helps ensure that the data types are correct and required columns are present, preventing bad data from causing data corruption.\n",
    "\n",
    "Table History and Time Travel\n",
    "- Delta Lake transaction log records details about every change made to data providing a full audit trail of the changes. \n",
    "- You can query previous snapshots of data enabling developers to access and revert to earlier versions of data for audits, rollbacks or to reproduce experiments.\n",
    "\n",
    "Delete data and Vacuum old versions\n",
    "- Delete data from tables using a predicate.\n",
    "- Fully remove data from previous versions using Vaccum to save storage and satisfy compliance requirements.\n",
    "\n",
    "Upsert data using Merge\n",
    "- Upsert data into tables from batch and streaming workloads\n",
    "- Use extended merge syntax for advanced usecases like data deduplication, change data capture, SCD type 2 operations, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a6563",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cleanup -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b80082-85c7-4de4-902e-85702ce7ce93",
   "metadata": {},
   "source": [
    "## Verify the Spark cluster PODs are terminating for your Spark Livy session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e3a8a-a344-47e4-8d9c-bf7bd6bd2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005bb610-f288-4939-ac9f-609212cce860",
   "metadata": {},
   "source": [
    "Delete the participant's folder in the default DataTap storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fed463-f287-4af3-92a4-025ae7fa1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "userID=\"student954\"\n",
    "hadoop fs -rm -r dtap://TenantStorage/${userID}\n",
    "hadoop fs -ls dtap://TenantStorage/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
